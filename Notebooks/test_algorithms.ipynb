{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Major Algorithms\n",
    "\n",
    "Regret analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.rcParams['font.size'] = 13\n",
    "plt.style.use(\"ggplot\")\n",
    "mpl.rcParams['font.family'] = \"Osaka\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common framework\n",
    "\n",
    "Consider the case that there are 3 arms: $A = \\{a_1, a_2, a_3\\}$. At each round $t = 1, 2, \\ldots, T$, a player chooses an arm $a_t \\in A$ and gets a random reward $r_t$. We assume the reward is an i.i.d. sample from the bernouli distribution $Ber(p_a)$, where \n",
    "\n",
    "$$\n",
    "p_a = \\begin{cases}\n",
    "0.25 & \\text{if $a = a_1$} \\\\\n",
    "0.5 & \\text{if $a = a_2$} \\\\\n",
    "0.75 & \\text{if $a = a_3$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A player initially does not know the true value of $p_a$ but does know the average performance among the all arms: $\\mu = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MAB(object):\n",
    "    def __init__(self, n_arms, payoff_dists, opt_arm_expected_payoff, rs=None):\n",
    "        self.n_arms = n_arms\n",
    "        self.payoff_dists = payoff_dists\n",
    "        self.opt_payoff = opt_arm_expected_payoff\n",
    "        if rs is None:\n",
    "            self.seed = np.random.randint(2**32 - 1)\n",
    "            self.rs = np.random.RandomState(self.seed)\n",
    "        else:\n",
    "            self.rs = rs\n",
    "\n",
    "    def pull_arm(self, arm, n=1):\n",
    "        if not isinstance(arm, int) or arm < 0 or self.n_arms <= arm:\n",
    "            raise ValueError(arm)\n",
    "        \n",
    "        payoff = np.sum(self.payoff_dists[arm].rvs(n, random_state=self.rs))\n",
    "        regret = self.opt_payoff * n - payoff\n",
    "        return payoff, regret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0, arm=0, reward= 1.000, regret=-0.250\n",
      "t=1, arm=1, reward= 0.000, regret= 0.750\n",
      "t=2, arm=2, reward= 1.000, regret=-0.250\n",
      "t=3, arm=0, reward= 0.000, regret= 0.750\n",
      "t=4, arm=1, reward= 0.000, regret= 0.750\n",
      "t=5, arm=2, reward= 1.000, regret=-0.250\n",
      "t=6, arm=0, reward= 1.000, regret=-0.250\n",
      "t=7, arm=1, reward= 1.000, regret=-0.250\n",
      "t=8, arm=2, reward= 1.000, regret=-0.250\n",
      "t=9, arm=0, reward= 1.000, regret=-0.250\n"
     ]
    }
   ],
   "source": [
    "n_arms = 3\n",
    "dists = [st.bernoulli(0.25*(i+1)) for i in range(n_arms)]\n",
    "opt_arm_expected_payoff = max([d.mean() for d in dists]) # = 0.75\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff)\n",
    "\n",
    "for i in range(10):\n",
    "    arm = i % 3\n",
    "    p, r = mab.pull_arm(arm)\n",
    "    print(\"t={0:d}, arm={1:d}, reward={2: .3f}, regret={3: .3f}\".format(i, arm, p, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_ber(p, q, eps=1e-5):\n",
    "    # q=0 or 1 case\n",
    "    if q < eps or q > 1 - eps:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    # p=0 or 1 case\n",
    "    if p < eps:\n",
    "        return (1-p) * np.log((1-p)/(1-q)) \n",
    "    elif p > 1 - eps:\n",
    "        return p * np.log(p/q)\n",
    "    \n",
    "    return p * np.log(p/q) + (1-p) * np.log((1-p)/(1-q)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self, mab, ts_length, rs=None):\n",
    "        self.mab = mab\n",
    "        self.ts_length = ts_length\n",
    "        \n",
    "        if rs is None:\n",
    "            self.seed = np.random.randint(2**32 - 1)\n",
    "            self.rs = np.random.RandomState(self.seed)\n",
    "        else:\n",
    "            self.rs = rs\n",
    "        \n",
    "        self.n_arms = mab.n_arms\n",
    "        self.t = 0\n",
    "        self.n_pull = np.zeros(self.n_arms, dtype=int)\n",
    "        self.sum_payoffs = np.zeros(self.n_arms, dtype=float)\n",
    "        self.sum_regrets = np.zeros(self.n_arms, dtype=float)\n",
    "        \n",
    "    def proceed(self):\n",
    "        pass\n",
    "    \n",
    "    def get_average_score(self):\n",
    "        average_payoff = np.sum(self.sum_payoffs) / self.t\n",
    "        average_regret = np.sum(self.sum_regrets) / self.t\n",
    "        return average_payoff, average_regret\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore-First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExploreFirst(Policy):\n",
    "    \"\"\"\n",
    "    Implement the Explore-First policy.\n",
    "    \n",
    "    * ts_explore: the number of rounds which this algorithm uses for an exploration of EACH arm\n",
    "    \"\"\"\n",
    "    def __init__(self, mab, ts_length, ts_explore):\n",
    "        super().__init__(mab, ts_length)\n",
    "        \n",
    "        self.ts_explore = ts_explore\n",
    "        self.total_ts_explore = ts_explore * self.n_arms\n",
    "        if self.total_ts_explore >= self.ts_length:\n",
    "            raise ValueError(\n",
    "                \"Explore rounds (times n_arms) should be less than or equal to `ts_length`\")\n",
    "        \n",
    "        self.opt_arm = None\n",
    "        \n",
    "    def proceed(self):\n",
    "        if self.t >= self.ts_length:\n",
    "            raise ValueError(\"current time exceeds `ts_length`\")\n",
    "            \n",
    "        if self.t == self.total_ts_explore:\n",
    "            mean_payoff = self.sum_payoffs / self.n_pull\n",
    "            self.opt_arm = int(np.argmax(mean_payoff))\n",
    "            \n",
    "        if self.t < self.total_ts_explore:\n",
    "            arm = int(self.t // self.ts_explore)\n",
    "        else:\n",
    "            arm = int(self.opt_arm)\n",
    "        \n",
    "        p, r = self.mab.pull_arm(arm)\n",
    "        self.n_pull[arm] += 1\n",
    "        self.sum_payoffs[arm] += p\n",
    "        self.sum_regrets[arm] += r\n",
    "        self.t += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=00, n_pull=[ 5  5 40], sum_payoffs: [  2.   3.  32.], sum_regrets: [ 1.75  0.75 -2.  ], average payoff: 0.740, average regret: 0.010\n",
      "i=01, n_pull=[ 5 40  5], sum_payoffs: [  0.  24.   5.], sum_regrets: [ 3.75  6.   -1.25], average payoff: 0.580, average regret: 0.170\n",
      "i=02, n_pull=[40  5  5], sum_payoffs: [ 10.   2.   3.], sum_regrets: [ 20.     1.75   0.75], average payoff: 0.300, average regret: 0.450\n",
      "i=03, n_pull=[ 5 40  5], sum_payoffs: [  2.  24.   2.], sum_regrets: [ 1.75  6.    1.75], average payoff: 0.560, average regret: 0.190\n",
      "i=04, n_pull=[ 5  5 40], sum_payoffs: [  3.   1.  31.], sum_regrets: [ 0.75  2.75 -1.  ], average payoff: 0.700, average regret: 0.050\n",
      "i=05, n_pull=[ 5 40  5], sum_payoffs: [  1.  26.   3.], sum_regrets: [ 2.75  4.    0.75], average payoff: 0.600, average regret: 0.150\n",
      "i=06, n_pull=[ 5  5 40], sum_payoffs: [  1.   2.  27.], sum_regrets: [ 2.75  1.75  3.  ], average payoff: 0.600, average regret: 0.150\n",
      "i=07, n_pull=[ 5 40  5], sum_payoffs: [  1.  22.   4.], sum_regrets: [ 2.75  8.   -0.25], average payoff: 0.540, average regret: 0.210\n",
      "i=08, n_pull=[ 5 40  5], sum_payoffs: [  0.  23.   2.], sum_regrets: [ 3.75  7.    1.75], average payoff: 0.500, average regret: 0.250\n",
      "i=09, n_pull=[ 5 40  5], sum_payoffs: [  2.  19.   4.], sum_regrets: [  1.75  11.    -0.25], average payoff: 0.500, average regret: 0.250\n",
      "i=10, n_pull=[ 5  5 40], sum_payoffs: [  1.   3.  30.], sum_regrets: [ 2.75  0.75  0.  ], average payoff: 0.680, average regret: 0.070\n",
      "i=11, n_pull=[ 5  5 40], sum_payoffs: [  2.   2.  31.], sum_regrets: [ 1.75  1.75 -1.  ], average payoff: 0.700, average regret: 0.050\n",
      "i=12, n_pull=[ 5  5 40], sum_payoffs: [  2.   1.  30.], sum_regrets: [ 1.75  2.75  0.  ], average payoff: 0.660, average regret: 0.090\n",
      "i=13, n_pull=[ 5 40  5], sum_payoffs: [  1.  17.   3.], sum_regrets: [  2.75  13.     0.75], average payoff: 0.420, average regret: 0.330\n",
      "i=14, n_pull=[ 5  5 40], sum_payoffs: [  0.   3.  28.], sum_regrets: [ 3.75  0.75  2.  ], average payoff: 0.620, average regret: 0.130\n",
      "i=15, n_pull=[ 5  5 40], sum_payoffs: [  1.   2.  25.], sum_regrets: [ 2.75  1.75  5.  ], average payoff: 0.560, average regret: 0.190\n",
      "i=16, n_pull=[ 5  5 40], sum_payoffs: [  1.   1.  32.], sum_regrets: [ 2.75  2.75 -2.  ], average payoff: 0.680, average regret: 0.070\n",
      "i=17, n_pull=[ 5  5 40], sum_payoffs: [  2.   3.  26.], sum_regrets: [ 1.75  0.75  4.  ], average payoff: 0.620, average regret: 0.130\n",
      "i=18, n_pull=[ 5  5 40], sum_payoffs: [  2.   2.  28.], sum_regrets: [ 1.75  1.75  2.  ], average payoff: 0.640, average regret: 0.110\n",
      "i=19, n_pull=[ 5 40  5], sum_payoffs: [  1.  19.   3.], sum_regrets: [  2.75  11.     0.75], average payoff: 0.460, average regret: 0.290\n"
     ]
    }
   ],
   "source": [
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "ts_length = 50\n",
    "ts_explore = 5\n",
    "\n",
    "for i in range(20):\n",
    "    efp = ExploreFirst(mab, ts_length, ts_explore)\n",
    "\n",
    "    for t in range(ts_length):\n",
    "        efp.proceed()\n",
    "\n",
    "    p, r = efp.get_average_score()\n",
    "    \n",
    "    print(\n",
    "        \"i={:02d}, n_pull={}, sum_payoffs: {}, sum_regrets: {}, \"\n",
    "        \"average payoff: {:.3f}, average regret: {:.3f}\"\n",
    "        .format(i, efp.n_pull, efp.sum_payoffs, efp.sum_regrets, p, r)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalueate average regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.66314450678228509, 221)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_length = 1000\n",
    "c = 1/16\n",
    "epsilon = 2 * n_arms * np.log(ts_length) / (c * ts_length)\n",
    "ts_explore = int(epsilon * ts_length // n_arms)\n",
    "epsilon, ts_explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare simulated average regret and theoretical uppder bound of regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164.26 166.536126696\n"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "regrets = np.zeros(size)\n",
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "\n",
    "for i in range(size):\n",
    "    efp = ExploreFirst(mab, ts_length, ts_explore)\n",
    "    \n",
    "    for t in range(ts_length):\n",
    "        efp.proceed()\n",
    "\n",
    "    p, r = efp.get_average_score()\n",
    "    regrets[i] = r\n",
    "\n",
    "regrets *= ts_length\n",
    "sum_regret = np.mean(regrets)\n",
    "upper_bound_regret = (1/2 + 1/4) * (2 * np.log(ts_length) / c + 1)\n",
    "print(sum_regret, upper_bound_regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated mean: 166.42, Upper bound: 166.536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6MAAAIMCAYAAAAAW3qTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1103af8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "plt.hist(regrets, color=\"#0000BB\")\n",
    "plt.plot([upper_bound_regret, upper_bound_regret], [0, 22], color=\"#EEFF00\", linewidth=\"3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy(Policy):\n",
    "    \"\"\"\n",
    "    Implement the Epsilon-Greedy policy.\n",
    "    \n",
    "    * ts_explore: the number of rounds which this algorithm uses for an exploration of EACH arm\n",
    "    \"\"\"\n",
    "    def __init__(self, mab, ts_length, ts_explore):\n",
    "        super().__init__(mab, ts_length)\n",
    "        \n",
    "        self.ts_explore = ts_explore\n",
    "        self.total_ts_explore = ts_explore * self.n_arms\n",
    "        if self.total_ts_explore >= self.ts_length:\n",
    "            raise ValueError(\n",
    "                \"Explore rounds (times n_arms) should be less than or equal to `ts_length`\")\n",
    "        \n",
    "        self.opt_arm = None\n",
    "        \n",
    "    def proceed(self):\n",
    "        if self.t >= self.ts_length:\n",
    "            raise ValueError(\"current time exceeds `ts_length`\")\n",
    "            \n",
    "        if self.t < self.total_ts_explore:\n",
    "            arm = int(self.t // self.ts_explore)\n",
    "        else:\n",
    "            mean_payoff = self.sum_payoffs / self.n_pull\n",
    "            arm = int(np.argmax(mean_payoff))\n",
    "        \n",
    "        p, r = self.mab.pull_arm(arm)\n",
    "        self.n_pull[arm] += 1\n",
    "        self.sum_payoffs[arm] += p\n",
    "        self.sum_regrets[arm] += r\n",
    "        self.t += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute simulated average regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164.26\n"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "regrets = np.zeros(size)\n",
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "\n",
    "for i in range(size):\n",
    "    eps = EpsilonGreedy(mab, ts_length, ts_explore)\n",
    "    \n",
    "    for t in range(ts_length):\n",
    "        eps.proceed()\n",
    "\n",
    "    p, r = eps.get_average_score()\n",
    "    regrets[i] = r\n",
    "\n",
    "regrets *= ts_length\n",
    "sum_regret = np.mean(regrets)\n",
    "print(sum_regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In this example) simulate average regret is same as that of Explore-First case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UCB1(Policy):\n",
    "    \"\"\"\n",
    "    Implement the UCB1 policy.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, mab, ts_length):\n",
    "        super().__init__(mab, ts_length)\n",
    "        \n",
    "    def proceed(self):\n",
    "        # choose an arm\n",
    "        if self.t < self.n_arms:\n",
    "            arm = self.t\n",
    "        else:\n",
    "            values = [\n",
    "                self.get_score(self.t, self.sum_payoffs[a], self.n_pull[a]) for a in range(self.n_arms)]\n",
    "            arm = int(np.argmax(values))\n",
    "        \n",
    "        p, r = mab.pull_arm(arm)\n",
    "        self.n_pull[arm] += 1\n",
    "        self.sum_payoffs[arm] += p\n",
    "        self.sum_regrets[arm] += r\n",
    "        self.t += 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_score(t, total, size):\n",
    "        return total/size + np.sqrt(np.log(t) / (2 * size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many times each arm is pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=20, n_pull=[ 5 11  4], sum_payoffs: [ 2.  7.  1.], sum_regrets: [ 1.75  1.25  2.  ]\n",
      "t=40, n_pull=[ 7 22 11], sum_payoffs: [  2.  11.   5.], sum_regrets: [ 3.25  5.5   3.25]\n",
      "t=60, n_pull=[ 7 22 31], sum_payoffs: [  2.  11.  20.], sum_regrets: [ 3.25  5.5   3.25]\n",
      "t=80, n_pull=[ 7 22 51], sum_payoffs: [  2.  11.  38.], sum_regrets: [ 3.25  5.5   0.25]\n",
      "t=100, n_pull=[ 7 22 71], sum_payoffs: [  2.  11.  54.], sum_regrets: [ 3.25  5.5  -0.75]\n",
      "t=120, n_pull=[ 7 22 91], sum_payoffs: [  2.  11.  67.], sum_regrets: [ 3.25  5.5   1.25]\n",
      "t=140, n_pull=[  7  22 111], sum_payoffs: [  2.  11.  83.], sum_regrets: [ 3.25  5.5   0.25]\n",
      "t=160, n_pull=[ 11  22 127], sum_payoffs: [  4.  11.  94.], sum_regrets: [ 4.25  5.5   1.25]\n",
      "t=180, n_pull=[ 12  22 146], sum_payoffs: [   4.   11.  104.], sum_regrets: [ 5.   5.5  5.5]\n",
      "t=200, n_pull=[ 12  24 164], sum_payoffs: [   4.   12.  119.], sum_regrets: [ 5.  6.  4.]\n",
      "t=220, n_pull=[ 12  24 184], sum_payoffs: [   4.   12.  134.], sum_regrets: [ 5.  6.  4.]\n",
      "t=240, n_pull=[ 12  24 204], sum_payoffs: [   4.   12.  148.], sum_regrets: [ 5.  6.  5.]\n",
      "t=260, n_pull=[ 12  24 224], sum_payoffs: [   4.   12.  165.], sum_regrets: [ 5.  6.  3.]\n",
      "t=280, n_pull=[ 12  24 244], sum_payoffs: [   4.   12.  180.], sum_regrets: [ 5.  6.  3.]\n",
      "t=300, n_pull=[ 12  26 262], sum_payoffs: [   4.   13.  193.], sum_regrets: [ 5.   6.5  3.5]\n"
     ]
    }
   ],
   "source": [
    "ts_length = 300\n",
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "\n",
    "ucb1 = UCB1(mab, ts_length)\n",
    "for t in range(ts_length):\n",
    "    ucb1.proceed()\n",
    "    p, r = ucb1.get_average_score()\n",
    "    if t % 20 == 19:\n",
    "        print(\n",
    "            \"t={:02d}, n_pull={}, sum_payoffs: {}, sum_regrets: {}\"\n",
    "            .format(ucb1.t, ucb1.n_pull, ucb1.sum_payoffs, ucb1.sum_regrets)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistency, non-optimal arms should be pulled at least $O(\\log t)$ times by $t$ period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7037824746562009"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare simulated average regret / theoretical uppder bound of regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.79\n"
     ]
    }
   ],
   "source": [
    "ts_length = 1000\n",
    "size = 100\n",
    "regrets = np.zeros(size)\n",
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "\n",
    "for i in range(size):\n",
    "    ucb1 = UCB1(mab, ts_length)\n",
    "    \n",
    "    for t in range(ts_length):\n",
    "        ucb1.proceed()\n",
    "\n",
    "    p, r = ucb1.get_average_score()\n",
    "    regrets[i] = r\n",
    "\n",
    "regrets *= ts_length\n",
    "sum_regret = np.mean(regrets)\n",
    "\n",
    "\n",
    "print(sum_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.79 20.7232658369\n"
     ]
    }
   ],
   "source": [
    "upper_bound_regret = np.log(ts_length) / 2 / (1/2) + np.log(ts_length) / 2 / (1/4)\n",
    "print(sum_regret, upper_bound_regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DMED(Policy):\n",
    "    \"\"\"\n",
    "    Implement the DMED policy.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, mab, ts_length):\n",
    "        super().__init__(mab, ts_length)\n",
    "\n",
    "        self.current_loop_arms = np.ones(self.n_arms, dtype=bool)\n",
    "        self.next_loop_arms = np.zeros(self.n_arms, dtype=bool)\n",
    "        \n",
    "    def proceed(self):\n",
    "        if self.t >= self.ts_length:\n",
    "            raise ValueError(\"current time exceeds `ts_length`\")\n",
    "            \n",
    "        # choose an arm\n",
    "        if self.t < self.n_arms:\n",
    "            arm = self.t\n",
    "        else:\n",
    "            if np.sum(self.current_loop_arms) == 0:\n",
    "                self.current_loop_arms, self.next_loop_arms \\\n",
    "                    = self.next_loop_arms, self.current_loop_arms\n",
    "            \n",
    "            for a in range(self.n_arms):\n",
    "                if self.current_loop_arms[a]:\n",
    "                    self.current_loop_arms[a] = False\n",
    "                    arm = a\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(self.current_loop_arms, self.next_loop_arms)\n",
    "            \n",
    "        p, r = mab.pull_arm(arm)\n",
    "        self.n_pull[arm] += 1\n",
    "        self.sum_payoffs[arm] += p\n",
    "        self.sum_regrets[arm] += r\n",
    "        \n",
    "        # update arms for next loop\n",
    "        if self.n_arms <= self.t:\n",
    "            aves = self.sum_payoffs / self.n_pull\n",
    "            opt_arm = np.argmax(aves)\n",
    "            highest_ave = aves[opt_arm]\n",
    "            \n",
    "            score_criterion = np.log(self.t)\n",
    "            for a in range(self.n_arms):\n",
    "                # handling kl divergence = infinity case\n",
    "                if a == opt_arm:\n",
    "                    self.next_loop_arms[a] = True\n",
    "                else:\n",
    "                    score = self.n_pull[a] * kl_ber(aves[a], highest_ave)\n",
    "                    if score <= score_criterion:\n",
    "                        self.next_loop_arms[a] = True\n",
    "            \n",
    "        self.t += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistency, non-optimal arms should be pulled at least $O(\\log t)$ times by $t$ period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=20, n_pull=[6 7 7], sum_payoffs: [ 2.  6.  5.], sum_regrets: [ 2.5  -0.75  0.25]\n",
      "t=40, n_pull=[10 15 15], sum_payoffs: [ 2.  9.  8.], sum_regrets: [ 5.5   2.25  3.25]\n",
      "t=60, n_pull=[12 24 24], sum_payoffs: [  2.  14.  14.], sum_regrets: [ 7.  4.  4.]\n",
      "t=80, n_pull=[12 34 34], sum_payoffs: [  2.  15.  22.], sum_regrets: [  7.   10.5   3.5]\n",
      "t=100, n_pull=[12 44 44], sum_payoffs: [  2.  21.  29.], sum_regrets: [  7.  12.   4.]\n",
      "t=120, n_pull=[12 54 54], sum_payoffs: [  2.  26.  37.], sum_regrets: [  7.   14.5   3.5]\n",
      "t=140, n_pull=[12 58 70], sum_payoffs: [  2.  28.  50.], sum_regrets: [  7.   15.5   2.5]\n",
      "t=160, n_pull=[12 58 90], sum_payoffs: [  2.  28.  63.], sum_regrets: [  7.   15.5   4.5]\n",
      "t=180, n_pull=[ 12  64 104], sum_payoffs: [  2.  31.  70.], sum_regrets: [  7.  17.   8.]\n",
      "t=200, n_pull=[ 12  67 121], sum_payoffs: [  2.  32.  84.], sum_regrets: [  7.    18.25   6.75]\n",
      "t=220, n_pull=[ 12  67 141], sum_payoffs: [  2.  32.  99.], sum_regrets: [  7.    18.25   6.75]\n",
      "t=240, n_pull=[ 12  67 161], sum_payoffs: [   2.   32.  113.], sum_regrets: [  7.    18.25   7.75]\n",
      "t=260, n_pull=[ 12  67 181], sum_payoffs: [   2.   32.  130.], sum_regrets: [  7.    18.25   5.75]\n",
      "t=280, n_pull=[ 12  67 201], sum_payoffs: [   2.   32.  145.], sum_regrets: [  7.    18.25   5.75]\n",
      "t=300, n_pull=[ 12  67 221], sum_payoffs: [   2.   32.  160.], sum_regrets: [  7.    18.25   5.75]\n"
     ]
    }
   ],
   "source": [
    "ts_length = 300\n",
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "\n",
    "dmed = DMED(mab, ts_length)\n",
    "for t in range(ts_length):\n",
    "    dmed.proceed()\n",
    "    p, r = dmed.get_average_score()\n",
    "    if t % 20 == 19:\n",
    "        print(\n",
    "            \"t={:02d}, n_pull={}, sum_payoffs: {}, sum_regrets: {}\"\n",
    "            .format(dmed.t, dmed.n_pull, dmed.sum_payoffs, dmed.sum_regrets)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7037824746562009"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare simulated average regret / theoretical uppder bound of regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n"
     ]
    }
   ],
   "source": [
    "ts_length = 1000\n",
    "size = 100\n",
    "regrets = np.zeros(size)\n",
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "\n",
    "for i in range(size):\n",
    "    dmed = DMED(mab, ts_length)\n",
    "    \n",
    "    for t in range(ts_length):\n",
    "        dmed.proceed()\n",
    "\n",
    "    p, r = ucb1.get_average_score()\n",
    "    regrets[i] = r\n",
    "\n",
    "regrets *= ts_length\n",
    "sum_regret = np.mean(regrets)\n",
    "\n",
    "\n",
    "print(sum_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0 27.1556232504\n"
     ]
    }
   ],
   "source": [
    "upper_bound_regret = np.log(ts_length) * (1/4 / kl_ber(1/4, 3/4) + 1/2 / kl_ber(1/2, 3/4))\n",
    "print(sum_regret, upper_bound_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.549306144334 0.143841036226\n"
     ]
    }
   ],
   "source": [
    "print(kl_ber(1/4, 3/4), kl_ber(1/2, 3/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Thompson Sampling\n",
    "\n",
    "prior: $Beta(1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThompsonSampling(Policy):\n",
    "    \"\"\"\n",
    "    Implement the Thompson-Sampling policy.\n",
    "    \n",
    "    * priors: prior distributions of the rewards of arms\n",
    "    * update_func: function that updates prior distributions\n",
    "    \"\"\"\n",
    "    def __init__(self, mab, ts_length, priors, update_func):\n",
    "        super().__init__(mab, ts_length)\n",
    "        \n",
    "        if len(priors) != mab.n_arms:\n",
    "            raise ValueError\n",
    "        \n",
    "        self.priors = priors\n",
    "        self.update_func = update_func\n",
    "        \n",
    "    def update_priors(self, arm, payoff):\n",
    "        self.priors = self.update_func(self.priors, arm, payoff)\n",
    "        \n",
    "    def proceed(self):\n",
    "        if self.t >= self.ts_length:\n",
    "            raise ValueError(\"current time exceeds `ts_length`\")\n",
    "            \n",
    "        # choose an arm\n",
    "        values = [priors[a].rvs(random_state=self.rs) for a in range(self.n_arms)]\n",
    "        arm = int(np.argmax(values))\n",
    "        \n",
    "        p, r = mab.pull_arm(arm)\n",
    "        self.n_pull[arm] += 1\n",
    "        self.sum_payoffs[arm] += p\n",
    "        self.sum_regrets[arm] += r\n",
    "        self.update_priors(arm, p)\n",
    "        self.t += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistency, non-optimal arms should be pulled at least $O(\\log t)$ times by $t$ period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=20, n_pull=[ 3 13  4], sum_payoffs: [ 0.  8.  1.], sum_regrets: [ 2.25  1.75  2.  ], beta_expectation: [ 0.2    0.6    0.333]\n",
      "t=40, n_pull=[ 4 28  8], sum_payoffs: [  0.  16.   3.], sum_regrets: [ 3.  5.  3.], beta_expectation: [ 0.167  0.567  0.4  ]\n",
      "t=60, n_pull=[ 4 47  9], sum_payoffs: [  0.  28.   3.], sum_regrets: [ 3.    7.25  3.75], beta_expectation: [ 0.167  0.592  0.364]\n",
      "t=80, n_pull=[ 4 64 12], sum_payoffs: [  0.  33.   6.], sum_regrets: [  3.  15.   3.], beta_expectation: [ 0.167  0.515  0.5  ]\n",
      "t=100, n_pull=[ 4 71 25], sum_payoffs: [  0.  39.  18.], sum_regrets: [  3.    14.25   0.75], beta_expectation: [ 0.167  0.548  0.704]\n",
      "t=120, n_pull=[ 4 71 45], sum_payoffs: [  0.  39.  31.], sum_regrets: [  3.    14.25   2.75], beta_expectation: [ 0.167  0.548  0.681]\n",
      "t=140, n_pull=[ 4 73 63], sum_payoffs: [  0.  41.  46.], sum_regrets: [  3.    13.75   1.25], beta_expectation: [ 0.167  0.56   0.723]\n",
      "t=160, n_pull=[ 4 73 83], sum_payoffs: [  0.  41.  59.], sum_regrets: [  3.    13.75   3.25], beta_expectation: [ 0.167  0.56   0.706]\n",
      "t=180, n_pull=[  4  75 101], sum_payoffs: [  0.  42.  69.], sum_regrets: [  3.    14.25   6.75], beta_expectation: [ 0.167  0.558  0.68 ]\n",
      "t=200, n_pull=[  4  76 120], sum_payoffs: [  0.  43.  85.], sum_regrets: [  3.  14.   5.], beta_expectation: [ 0.167  0.564  0.705]\n",
      "t=220, n_pull=[  4  76 140], sum_payoffs: [   0.   43.  100.], sum_regrets: [  3.  14.   5.], beta_expectation: [ 0.167  0.564  0.711]\n",
      "t=240, n_pull=[  4  76 160], sum_payoffs: [   0.   43.  114.], sum_regrets: [  3.  14.   6.], beta_expectation: [ 0.167  0.564  0.71 ]\n",
      "t=260, n_pull=[  4  76 180], sum_payoffs: [   0.   43.  131.], sum_regrets: [  3.  14.   4.], beta_expectation: [ 0.167  0.564  0.725]\n",
      "t=280, n_pull=[  4  77 199], sum_payoffs: [   0.   44.  145.], sum_regrets: [  3.    13.75   4.25], beta_expectation: [ 0.167  0.57   0.726]\n",
      "t=300, n_pull=[  5  77 218], sum_payoffs: [   1.   44.  160.], sum_regrets: [  2.75  13.75   3.5 ], beta_expectation: [ 0.286  0.57   0.732]\n"
     ]
    }
   ],
   "source": [
    "ts_length = 300\n",
    "priors = [st.beta(1, 1) for i in range(n_arms)]\n",
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "\n",
    "def update_priors(priors, arm, payoff):\n",
    "    if payoff == 1:\n",
    "        a = priors[arm].args[0] + 1\n",
    "        b = priors[arm].args[1]\n",
    "    elif payoff == 0:\n",
    "        a = priors[arm].args[0]\n",
    "        b = priors[arm].args[1] + 1\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    priors[arm].args = (a, b)\n",
    "    return priors\n",
    "\n",
    "thompson = ThompsonSampling(mab, ts_length, priors, update_priors)\n",
    "for t in range(ts_length):\n",
    "    thompson.proceed()\n",
    "    p, r = thompson.get_average_score()\n",
    "    if t % 20 == 19:\n",
    "        beta_exp = np.array([thompson.priors[a].mean() for a in range(thompson.n_arms)])\n",
    "        print(\n",
    "            \"t={:02d}, n_pull={}, sum_payoffs: {}, sum_regrets: {}, beta_expectation: {}\"\n",
    "            .format(thompson.t, thompson.n_pull, thompson.sum_payoffs, thompson.sum_regrets, beta_exp)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts_length = 1000\n",
    "size = 100\n",
    "regrets = np.zeros(size)\n",
    "rs = np.random.RandomState(seed)\n",
    "mab = MAB(n_arms, dists, opt_arm_expected_payoff, rs=rs)\n",
    "\n",
    "for i in range(size):\n",
    "    thompson = ThompsonSampling(mab, ts_length, priors, update_priors)\n",
    "    \n",
    "    for t in range(ts_length):\n",
    "        thompson.proceed()\n",
    "\n",
    "    p, r = thompson.get_average_score()\n",
    "    regrets[i] = r\n",
    "\n",
    "regrets *= ts_length\n",
    "sum_regret = np.mean(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare simulated average regret / theoretical uppder bound of regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49 27.1556232504\n"
     ]
    }
   ],
   "source": [
    "upper_bound_regret = np.log(ts_length) * (1/4 / kl_ber(1/4, 3/4) + 1/2 / kl_ber(1/2, 3/4))\n",
    "print(sum_regret, upper_bound_regret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
